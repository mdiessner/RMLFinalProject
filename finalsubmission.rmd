---
title: "R Machine Learning Final Project"
author: "Martin Diessner"
date: "10.03.2022"
output: html_document
---

# Instructions

One thing that people regularly do is quantify how  much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants.
We will use the "classe" as our dependent variable and select other variables or the full given dataset as independent variables.

# Code

### Import libraries
```r 

library(caret)
library(randomForest)
library(xgboost)
library(dplyr)

```

### Load training and testing datasets

```r

training <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv")
testing <- read.csv("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv")

```

### Clean training and test sets
#### As the testing set has more NA columns, we remove the NA columns from training as well
```r
training <- training[,8:160]
testing <- testing[,8:160]
training <- training[,colSums(is.na(testing))==0]
testing <- testing[,colSums(is.na(testing))==0]
```

### Split testing set 80/20 for initial assessment of testing set
```r
set.seed(1000)
inTrain<-createDataPartition(training$classe, p=0.8, list=FALSE)
training.train<-training[inTrain,]
training.test<-training[-inTrain,]
```

### Find highly correlated variables and eliminate from model-fitting
```r
cor_matrix <- cor(training.train[,-which(names(training.train) == "classe")])
cor_matrix_rm <- cor_matrix                  # Modify correlation matrix
cor_matrix_rm[upper.tri(cor_matrix_rm)] <- 0
diag(cor_matrix_rm) <- 0
cor_matrix_rm

training.classe = training.train$classe
training.train <- training.train[, !apply(cor_matrix_rm, 2, function(x) any(x > 0.8))]
training.train$classe = factor(training.classe)
training.test$classe = factor(training.test$classe)
training.train <- training.train[!is.na(training.train$classe),]

training.test <- select(training.test, colnames(training.train))
```

### Train different models (using rf, gbm, lda and xgboost)
```r

rfmodel <- randomForest(classe ~ ., data=training.train)
gbmmodel <- train(classe ~ ., method="gbm", data=training.train)
ldamodel <- train(classe ~ ., method="lda", data=training.train)
xgdata <- data.matrix(training.train[-which(names(training.train) == "classe")])
xgmodel <- xgboost(data=xgdata, label = as.integer(training.train$classe), nthread=4, nrounds=300)
```

### Predict outcomes from all three models except XGBoost using test data
```r

rfprediction <- predict(rfmodel, training.test)
gbmprediction <- predict(gbmmodel, training.test)
ldaprediction <- predict(ldamodel, training.test)
```

### Show confusion matrix to evaluate 
```r

confusionMatrix(rfprediction, training.test$classe)
confusionMatrix(gbmprediction, training.test$classe)
confusionMatrix(ldaprediction, training.test$classe)
```
* RandomForest has an Accuracy of 0.9941 and therefore performs best
* GBM shows an accuracy of 0.9605
* LDA performs considerably worse at 0.6691

### XGBoost Evaluation 
#### (I am not that firm on XGBoost, so this is more of an addon for me to train). It also does not perform as well as other)
```r

xgpredict <- predict(xgmodel, data.matrix(training.test[-which(names(training.train) == "classe")]))
xgpredict <- as.integer(round(xgpredict, digits =0))
xgpredictf <- as.character(xgpredict)
xgpredictf[xgpredictf=="0"] <- "A"
xgpredictf[xgpredictf=="1"] <- "A"
xgpredictf[xgpredictf=="2"] <- "B"
xgpredictf[xgpredictf=="3"] <- "C"
xgpredictf[xgpredictf=="4"] <- "D"
xgpredictf[xgpredictf=="5"] <- "E"
confusionMatrix(factor(xgpredictf), training.test$classe)
```
Accuracy of 0.9587, therefore not performing quite well, but still below the other two algorithms, at nrounds > 500 performs better than gbm

# Sources
Data generously prvoided from http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har